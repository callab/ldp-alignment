---
title: "Linguistic Alignment in LDP"
author: "Jo Denby and Dan Yurovsky"
date: '`r Sys.Date()`'
output:
  pdf_document:
  toc: no
html_document:
  code_folding: show
number_sections: no
theme: lumen
toc: no
toc_float: no
---
  
```{r setup, include = FALSE}
library(knitr)
library(tidyverse)
library(DBI)
library(here)
library(data.table)
library(tidytext)
library(RMySQL)
library(feather)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = TRUE, tidy = FALSE)

# LDP auxiliary functions
source(here("read_ldp.R"))
```

Read in conversation data
```{r, eval=T, message=FALSE}
MIN_VISITS <- 5

Mode <- function(x) {
  x <- x[!is.na(x)]
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

#### Load and Clean Data
# Read in LDP data
ldp <- connect_to_ldp()

# subject demographics
demos <- get_table(ldp, "subjects") %>%
        filter(lesion == "") %>%
        select(id, sex, race, ethn) %>%
        collect()

# visits information and more subject demographics
visits <- get_table(ldp, "home_visits") %>%
  distinct(id, age_years, subject, visit_type, completed, income_category, 
           mother_education) %>%
  collect() %>%
  filter(visit_type != "SB5") %>%
  mutate(visit_type = as.numeric(gsub("[^0-9]", "", visit_type))) %>%
  rename(visit = visit_type) %>%
  mutate(completed = as.logical(completed),
         income_category = as.numeric(income_category))

subjs <- visits %>%
  group_by(subject) %>%
  summarise(completed = sum(completed),
            income_category = Mode(income_category),
            mother_education = Mode(mother_education)) %>%
  filter(completed >= 5) %>%
  select(-completed) %>%
  right_join(demos, by = c("subject" = "id"))

session_ages <- visits %>%
        filter(subject %in% subjs$subject, completed) %>%
        select(subject, visit, age_years) %>%
        rename(age = age_years) %>%
        arrange(subject, visit)
```

Get other measures
```{r load_measures}
ppvt <- get_table(ldp, "ppvt") %>%
  collect() %>%
  select(-id) %>%
  rename(id = visit) %>%
  left_join(visits) %>%
  filter(subject %in% subjs$subject) %>%
  rename(ppvt = ppvt_raw) %>%
  select(subject, visit, age_years, ppvt)
  
cdi_ws <- get_table(ldp, "cdi_words_and_sentences") %>%
  select(id, visit, cdis_1a1_num, cdis_2e_num) %>%
  collect() %>%
  select(-id) %>%
  rename(id = visit) %>%
  left_join(visits) %>%
  filter(subject %in% subjs$subject) %>%
  rename(cdi_ws = cdis_1a1_num, sent_comp = cdis_2e_num) %>%
  select(subject, visit, age_years, cdi_ws, sent_comp) %>%
  arrange(subject, visit)

syntax <- get_table(ldp, "syntax") %>%
  collect() %>%
  select(-id) %>%
  rename(id = visit) %>%
  left_join(visits) %>%
  filter(subject %in% subjs$subject) %>%
  rename(syntax = syntax_raw) %>%
  select(subject, visit, age_years, syntax) %>% 
  arrange(subject, visit)

wppsi <- get_table(ldp, "wppsi_block_design") %>%
  collect() %>%
  select(-id) %>%
  rename(id = visit) %>%
  left_join(visits) %>%
  filter(subject %in% subjs$subject) %>%
  arrange(subject, visit) %>%
  filter(!is.na(wppsi_block_raw)) %>%
  select(subject, visit, age_years, wppsi_block_raw) %>%
  rename(wppsi_block = wppsi_block_raw)

measures <- full_join(ppvt, cdi_ws, by = c("subject", "visit", "age_years")) %>%
  full_join(syntax, by = c("subject", "visit", "age_years")) %>%
  full_join(wppsi, by = c("subject", "visit", "age_years")) %>%
  gather(measure, value, ppvt, cdi_ws, sent_comp, syntax, wppsi_block) %>%
  filter(!is.na(value)) %>%
  arrange(subject, measure, visit) %>%
  mutate(person = "child")
```

Read in LIWC Categories
```{r read_liwc}
liwc <- read_tsv("word_lists/liwc2007_converted.tsv", col_names = FALSE) %>%
  rename(word = X1, category = X2)
```

```{r Establish Look-Up Table for LIWC Word Values}
# To deal with multi-category words (MALLEABLE)

liwc_points <- liwc %>%
  group_by(word) %>% 
  mutate(n = n(),
         points= 1/n)

```


Set up utterances for computing alignment
```{r setup_alignment}
# read data from sql connection
utterances <- tbl(ldp, "utterances") %>%
  select(subject, session, line, p_chat, c_chat) %>%
  filter(p_chat != "" | c_chat != "") %>%
  collect() %>%
  filter(subject %in% unique(visits$id)) %>%
  mutate(order = 1:n())
  
# better organization of chat by person, subject, session, run
split_utts <- utterances %>%
  gather(person, chat, c(p_chat, c_chat)) %>%
  mutate(person = if_else(person == "p_chat", "Parent", "Child")) %>%
  filter(chat != "") %>%
  arrange(order) %>%
  mutate(run = rleid(subject, session, person)) %>%
  group_by(run, subject, session, person) %>%
  summarise(chat = paste0(chat, collapse = " ")) 

#splits up run utterances into individual token words
tokens <- split_utts %>%
  unnest_tokens(word, chat) %>%
  arrange(run, subject, session) %>%
  group_by(subject, session)

# Get length of every utterance
split_utts <- split_utts %>% 
  mutate(length = str_count(chat, '\\+|\\s')) %>% 
  ungroup() %>% 
  group_by(subject, session,person) %>% 
  mutate(tokens = sum(length),
         mlu = mean(length))

# Compute type, token and mlu information
tokens_mlu <- split_utts %>% 
  select(subject,session,person,tokens,mlu) %>% 
  distinct()

types <- tokens %>% 
  ungroup() %>% 
  group_by(subject,session,person) %>% 
  summarise(types = n_distinct(word)) 

tt_mlu <- types %>% 
  left_join(tokens_mlu, by= c('subject','session','person'))
```

The chunk below is the bulk of preprocessing. It expects two dataframes: 

1. `liwc_points`: contains every word in the liwc dataset with its corresponding category (or categories, on separate rows), and 'points' value (computed by the reciprocal of the number of categories in which a word appears.)

2. `tokens`: utterance data separated by each word with its correspoding person ('Parent' or 'Child'), session number, subject number, and run number (to track utterance order within sessions)

It produces `long_present`, a dataframe that contains every utterance in the corpus alongside computed counts for each LIWC category in that utterance.

```{r Recompute Word Counts acc. to Multi-LIWC Values}
n_words <- nrow(liwc_points)

# check every token in corpus against every word/stem in liwc dataset
present <- map(1:n_words, function(row) {
  data_frame(present = grepl(paste0("^",liwc_points[row,"word"],'$'), tokens$word,ignore.case = TRUE))}) %>%
  bind_cols()
names(present) <- pull(liwc_points,word)[1:n_words]


# for memory reasons, split data into chunks
chunk <- 10000
n <- nrow(present)
r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
t <- split(tokens,r)
d <- split(present,r)

long_present <- data_frame()

# for each chunk, compute total LIWC occurrences in each utterance
for (j in 1:length(d)) {
  tokens_slice <- t[[j]]
  
  # Count up instances of LIWC words according to liwc_points
  temp <- d[[j]] %>%
    bind_cols(tokens_slice) %>%
    gather(liwc_word, count, names(present)) %>%
    left_join(liwc_points, by = c("liwc_word" = "word")) %>%
    ungroup() %>% 
    mutate(count = case_when(count == TRUE ~ points,
                           TRUE ~ 0)) %>%
    group_by(subject, session, person, category,run) %>%
    summarise(count = sum(count)) %>%
    arrange(run)

  long_present <- bind_rows(long_present,temp)
}
```

```{r Compute Alignment Information}

# This computes the values used for estimating alignment in the Stan models
long_present <- long_present %>%
  arrange(subject, session, category, run) %>%
  group_by(subject, session, category) %>%
  mutate(lag_count = lag(count)) %>%
  filter(!is.na(lag_count)) %>%
  group_by(subject, session, person, category) %>%
  mutate(BA = count > 0 & lag_count > 0,
         notBA = count == 0 & lag_count > 0,
         BnotA = count > 0 & lag_count == 0,
         notBnotA = count == 0 & lag_count == 0) %>%
  summarise_at(vars(BA,notBA,BnotA,notBnotA), sum) %>%
  mutate(NumUtterancesAB = BA +notBA,
         NumUtterancesNotAB = BnotA + notBnotA) %>%
  rename(CountsAB = BA, CountsNotAB = BnotA) %>%
    select(-notBA, - notBnotA)
```

```{r Join Demographic and PPVT Info and Pre-Process}
long_present_demos <- long_present %>%
  left_join(subjs, by = 'subject') %>%
  left_join(ppvt, by= 'subject') %>%
  na.omit() %>%
  mutate(female = as.numeric(sex == 'F'),
         male = as.numeric(sex == 'M'),
         white = as.numeric(race == 'WH'),
         black = as.numeric(race == 'BL'),
         multi = as.numeric(race == '2+'),
         income_category = income_category - 1)

# treat mother_ed as ordinal variable
conversion = c("Some High School"=0, "High School or GED"=1, "Some College or Trade School"=2, "Bachelor's Degree"=3, "Advanced Degree"=4)
long_present_demos$mother_education = conversion[long_present_demos$mother_education]

# Filter to only kids with >=10 sessions
subjs_over_10 <- long_present %>%
  select(subject,session) %>%
  distinct() %>%
  group_by(subject) %>%
  summarise(n=n()) %>%
  filter(n>=10) %>%
  pull(subject)

long_present <- long_present_demos %>%
  ungroup() %>%
  filter(subject %in% subjs_over_10) %>%
  mutate(uid = paste0(subject, "_", person)) %>%
  mutate(uid = as.numeric(as.factor(uid))) %>%
  mutate(parentid = case_when(person == 'Parent' ~ uid,
                              TRUE ~ uid + 1),
         childid = case_when(person == 'Child' ~ uid,
                             TRUE ~ uid - 1))


# write_feather(long_present, 'data/ppvt_with_demos.feather')

```

`long_present` at this stage is now ready for analysis via Stan.  

Analysis for paper contained within `alignment_correlations.Rmd`.