---
title: "Linguistic Alignment in LDP"
author: "Jo Denby, Ashley Leung, and Dan Yurovksy"
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: no
  html_document:
    code_folding: show
    number_sections: no
    theme: lumen
    toc: no
    toc_float: no
---

```{r setup, include = FALSE}
library(knitr)
library(data.table)
library(tidyverse)
library(readr)
library(stringr)
library(DT)
library(tidytext)
library(RSQLite)
library(feather)
library(xlsx)
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = TRUE, tidy = FALSE)
```


Read in LDP data. Don't display in public repo
```{r read_ldp}
LDP_DIR <- "~/Documents/LDP/ldp.db"
#LDP_DIR <- "~/ldp/data/ldp.db"

# Read in LDP data
ldp <- src_sqlite(LDP_DIR)

# Get all participants
subjs <- tbl(ldp, "subjects") %>%
  collect()

# Get visit data
visits <- tbl(ldp, "visits") %>%
  collect() %>%
  select(subject, session, date, child_age, child_age_years, child_age_months,
         income, mother_education, father_education)
```

Read in LIWC Categories
```{r read_liwc}
liwc <- read_tsv("word_lists/liwc2007_converted.tsv", col_names = FALSE) %>%
  rename(word = X1, category = X2)
```

Set up utterances for computing alignment
```{r setup_alignment}
# What do we do when both participants talk on the same line
utterances <- tbl(ldp, "utterances") %>%
  select(subject, session, line, p_chat, c_chat) %>%
  filter(p_chat != "" | c_chat != "") %>%
  collect() %>%
  mutate(order = 1:n())
  
#use rleid from data table to do squashing by run
split_utts <- utterances %>%
  gather(person, chat, c(p_chat, c_chat)) %>%
  mutate(person = if_else(person == "p_chat", "Parent", "Child")) %>%
  filter(chat != "") %>%
  arrange(order) %>%
  mutate(run = rleid(subject, session, person)) %>%
  group_by(run, subject, session, person) %>%
  summarise(chat = paste0(chat, collapse = " ")) 

#splits up run utterances into individual token words
tokens <- split_utts %>%
  unnest_tokens(word, chat) %>%
  arrange(run, subject, session) %>%
  group_by(subject, session)


# ^ asserts the beginning of pattern, $ asserts end; works with regular expressions as well
# ~ 15min
# present <- map(1:683, function(row) {
#   data_frame(present = grepl(paste0("^",liwc[row,"word"],'$'), tokens$word,ignore.case = TRUE))}) %>%
#   bind_cols()
# names(present) <- pull(liwc,word)[1:683]


long_present <- read_feather("~/Documents/ldp-alignment/long_present.feather")

long_present<-read_feather("long_present.feather")

## NEED TO REORDER AND REDO ALIGNMENT COMPUTATION ##
long_present <- long_present %>%
  arrange(subject,session,category,run) 



long_present <- long_present %>%
  mutate(
    BA = case_when(
      category == lag(category) ~ (ifelse(is.na(lag(count)),NA,pmin(count,lag(count), na.rm=TRUE))))) %>%
  mutate(
    notBA = case_when(
      category == lag(category) ~ (ifelse(is.na(lag(count)),NA,
                      ifelse(count>lag(count),count-lag(count),0))))) %>%
  mutate(
    BnotA = case_when(
      category == lag(category) ~ (ifelse(is.na(lag(count)),NA,
                      ifelse(lag(count)>count,lag(count)-count,0))))) %>%
  mutate(
    notBnotA = case_when(
      category == lag(category) ~ (ifelse(is.na(lag(count)),NA,
                         ifelse(lag(count)==0 & count==0,1,0)))))


liwc_markers <- as.numeric(as.factor(unique(liwc$category)))

###  STAN DATA ####
NumMarkers <- length(unique(liwc$category))
NumSubPops <- length(unique(long_present$person))
NumObservations <- dim(long_present)[1]
SpeakerSubPop <- as.numeric(as.factor(long_present$person)) # Child - 1; Parent - 2
SpeakerAge <- long_present$session
MarkerType <- as.numeric(as.factor(long_present$category)) #see liwc_markers
NumUtterancesAB <- rep(sum(long_present$notBA, na.rm=TRUE) + sum(long_present$notBnotA, na.rm=TRUE), NumObservations)
NumUtterancesNotAB <- rep(sum(long_present$BA, na.rm=TRUE) + sum(long_present$BnotA, na.rm=TRUE), NumObservations)
CountsAB <- rep(sum(long_present$notBA, na.rm=TRUE), NumObservations)
CountsNotAB <- rep(sum(long_present$BA, na.rm = TRUE),NumObservations)
StdDev <- 1
###################


####################################################

# # est time ~107 mins
# chunk <- 10000
# n <- nrow(present)
# r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
# d <- split(present,r)
# t <- split(tokens,r)

# long_present <- data_frame()
# 
# for (j in 1:length(d)) {
#   tokens_slice <- t[[j]]
#   
#   temp <- d[[j]] %>%
#   bind_cols(tokens_slice) %>%
#   gather(liwc_word, count, names(present)) %>%
#   left_join(liwc, by = c("liwc_word" = "word")) %>%
#   group_by(subject, session, person, category,run) %>%
#   summarise(count = sum(count)) %>%
#   arrange(run) %>%
#   group_by(subject,session, category)
#   
#   long_present <- bind_rows(long_present,temp)
# }

R_lesion_subjs <- subjs %>%
  filter(lesion=="R") %>%
  select(id)
L_lesion_subjs <- subjs %>%
  filter(lesion=="L") %>%
  select(id)

kid_income <- visits %>%
  group_by(subject) %>%
  summarise(income = mean(income, na.rm = T)) %>%
  ungroup() %>%
  mutate(upper_half = income > median(income, na.rm = T))

demographics <- read.xlsx("~/Desktop/LDP_Alignment/Demographics.xls",1) 

# only education data for 64/166 subjects
mother_ed <- demographics %>%
  select(c(subject_number, mother_education)) %>%
  filter(subject_number==subjs$id)

colnames(mother_ed) <- c('subject','mother_education')

long_present <- long_present %>%
  mutate(BA=ifelse(is.na(lag(count)),NA,pmin(count,lag(count), na.rm=TRUE))) %>%
  mutate(notBA=ifelse(is.na(lag(count)),NA,
                      ifelse(count>lag(count),count-lag(count),0))) %>%
  mutate(BnotA=ifelse(is.na(lag(count)),NA,
                      ifelse(lag(count)>count,lag(count)-count,0))) %>%
  mutate(notBnotA=ifelse(is.na(lag(count)),NA,
                         ifelse(lag(count)==0 & count==0,1,0))) %>%
  mutate(lesion=ifelse(subject %in% R_lesion_subjs$id,"R",
                       ifelse(subject %in% L_lesion_subjs$id,"L",''))) %>%
  left_join(kid_income)

long_present_ed <- long_present %>%
  filter(subject %in% ed_subjs) %>%
  left_join(mother_ed)

alignment_agg <- long_present %>%
  mutate_at(vars(BA, notBA, BnotA, notBnotA), 
            function(x) as.numeric(x > 0)) %>%
  gather(measure, value, BA, notBA, BnotA, notBnotA) %>%
  group_by(session, person, measure) %>%
  summarise(n = sum(value, na.rm = T)) %>% 
  spread(measure, n) %>%
  mutate(A = (BA + notBA) / (BA + notBA + BnotA + notBnotA),
         B = (BA + BnotA) / (BA + notBA + BnotA + notBnotA),
         PAB= BA/(BA + BnotA),
         PAnotB = notBA/(notBA + notBnotA),
         alignment = PAB - (PAB*B + PAnotB*(1-B))) %>%
  select(person, session, alignment) %>%
  filter(!is.na(alignment)) 

ggplot(data=alignment_agg, aes(x=as.factor(session), y=alignment, group=person)) +
  geom_line(stat='identity', aes(color=person)) + theme_bw() +
  xlab ('Session') +
  ylab ('Linguistic Alignment')

alignment_income <- long_present %>%
  mutate_at(vars(BA, notBA, BnotA, notBnotA), 
            function(x) as.numeric(x > 0)) %>%
  gather(measure, value, BA, notBA, BnotA, notBnotA) %>%
  group_by(session, upper_half, subject, person, measure) %>%
  summarise(n = sum(value, na.rm = T)) %>% 
  spread(measure, n) %>%
  mutate(A = (BA + notBA) / (BA + notBA + BnotA + notBnotA),
         B = (BA + BnotA) / (BA + notBA + BnotA + notBnotA),
         PAB= BA/(BA + BnotA),
         PAnotB = notBA/(notBA + notBnotA),
         alignment = PAB - (PAB*B + PAnotB*(1-B))) %>%
  select(session, person, upper_half, alignment, subject) %>%
  group_by(upper_half, session, person) %>%
  filter(!is.na(alignment))

avg_alignment_income <- alignment_income %>%
  group_by(upper_half, person, session) %>%
  summarise(sem = sd(alignment)/sqrt(n()), alignment = mean(alignment))


ggplot(data=avg_alignment_income, aes(x=as.factor(session),
                                      y=alignment, group=interaction(person,upper_half),
                                      color = person,
                                      fill = person)) +
  geom_pointrange(aes(ymin = alignment - sem, ymax = alignment + sem)) +
  geom_ribbon(aes(ymin = alignment - sem, ymax = alignment + sem), alpha = .1) +
  geom_line(stat='identity', aes(linetype=upper_half)) + theme_bw() +
  xlab ('Session') +
  ylab ('Linguistic Alignment')

alignment_ed <- long_present_ed %>%
  mutate_at(vars(BA, notBA, BnotA, notBnotA), 
            function(x) as.numeric(x > 0)) %>%
  gather(measure, value, BA, notBA, BnotA, notBnotA) %>%
  group_by(session, mother_education, person, measure) %>%
  summarise(n = sum(value, na.rm = T)) %>% 
  spread(measure, n) %>%
  mutate(A = (BA + notBA) / (BA + notBA + BnotA + notBnotA),
         B = (BA + BnotA) / (BA + notBA + BnotA + notBnotA),
         PAB= BA/(BA + BnotA),
         PAnotB = notBA/(notBA + notBnotA),
         alignment = PAB - (PAB*B + PAnotB*(1-B))) %>%
  select(session, person, mother_education, alignment) %>%
  filter(!is.na(alignment)) 

ggplot(data=alignment_ed, aes(x=as.factor(session), y=alignment, group=interaction(person, mother_education))) +
  geom_line(stat='identity', aes(color=person, linetype=mother_education)) + theme_bw() +
  xlab ('Session') +
  ylab ('Linguistic Alignment')+
  coord_cartesian(ylim=c(0,.15))

alignment_lesion <- long_present %>%
  mutate_at(vars(BA, notBA, BnotA, notBnotA), 
            function(x) as.numeric(x > 0)) %>%
  gather(measure, value, BA, notBA, BnotA, notBnotA) %>%
  group_by(session, lesion, person, measure) %>%
  summarise(n = sum(value, na.rm = T)) %>% 
  spread(measure, n) %>%
  mutate(A = (BA + notBA) / (BA + notBA + BnotA + notBnotA),
         B = (BA + BnotA) / (BA + notBA + BnotA + notBnotA),
         PAB= BA/(BA + BnotA),
         PAnotB = notBA/(notBA + notBnotA),
         alignment = PAB - (PAB*B + PAnotB*(1-B))) %>%
  select(session, person, lesion, alignment) %>%
  filter(!is.na(alignment)) 

ggplot(data=alignment_lesion, aes(x=as.factor(session), y=alignment, group=interaction(person,lesion) )) +
  geom_line(stat='identity', aes(color=person, linetype=lesion)) + theme_bw() +
  xlab ('Session') +
  ylab ('Linguistic Alignment')+
  coord_cartesian(ylim=c(0,.15))


#read in demographics data
#left join mother's education
#plot with that
#read through stan tutorial / introductions
```
