---
title: "Linguistic Alignment in LDP"
author: "Jo Denby, Ashley Leung, and Dan Yurovksy"
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: no
  html_document:
    code_folding: show
    number_sections: no
    theme: lumen
    toc: no
    toc_float: no
---

```{r setup, include = FALSE}
library(knitr)
library(data.table)
library(tidyverse)
library(readr)
library(stringr)
library(DT)
library(tidytext)
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = TRUE, tidy = FALSE)
```


Read in LDP data. Don't display in public repo
```{r read_ldp}
LDP_DIR <- "~/Documents/LDP/ldp.db"
#LDP_DIR <- "~/ldp/data/ldp.db"

# Read in LDP data
ldp <- src_sqlite(LDP_DIR)

# Get all participants
subjs <- tbl(ldp, "subjects") %>%
  collect()

# Get visit data
visits <- tbl(ldp, "visits") %>%
  collect() %>%
  select(subject, session, date, child_age, child_age_years, child_age_months,
         income, mother_education, father_education)
```

Read in LIWC Categories
```{r read_liwc}
liwc <- read_tsv("word_lists/liwc2007_converted.tsv", col_names = FALSE) %>%
  rename(word = X1, category = X2)
```

Set up utterances for computing alignment
```{r setup_alignment}
# What do we do when both participants talk on the same line
utterances <- tbl(ldp, "utterances") %>%
  select(subject, session, line, p_chat, c_chat) %>%
  filter(p_chat != "" | c_chat != "") %>%
  collect() %>%
  mutate(order = 1:n())
  
#use rleid from data table to do squashing by run
split_utts <- utterances %>%
  gather(person, chat, c(p_chat, c_chat)) %>%
  mutate(person = if_else(person == "p_chat", "Parent", "Child")) %>%
  filter(chat != "") %>%
  arrange(order) %>%
  mutate(run = rleid(subject, session, person)) %>%
  group_by(run, subject, session, person) %>%
  summarise(chat = paste0(chat, collapse = " ")) 

#splits up run utterances into individual token words
tokens <- split_utts %>%
  unnest_tokens(word, chat) %>%
  arrange(run, subject, session) %>%
  group_by(subject, session)

# ^ asserts the beginning of pattern, $ asserts end; works with regular expressions as well
# ~ 15min
present <- map(50:60, function(row) {
  data_frame(present = grepl(paste0("^",liwc[row,"word"],'$'), tokens$word,ignore.case = TRUE))}) %>%
  bind_cols()
names(present) <- pull(liwc,word)[50:60]

chunk <- 100
n <- nrow(present)
r  <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
d <- split(present,r)
t <- split(tokens,r)


long_present <- data.frame()
for (j in 1:100) {
  tokens_slice <- t[[j]]
  
  temp <- d[[j]] %>%
  bind_cols(tokens_slice) %>%
  gather(liwc_word, count, names(present)) %>%
  left_join(liwc, by = c("liwc_word" = "word")) %>%
  group_by(subject, session, person, category,run) %>%
  summarise(count = sum(count)) %>%
  arrange(run) %>%
  group_by(subject,session, category)
    
  long_present <- bind_rows(long_present,temp)
}

long_present <- long_present %>%
  mutate(AB=ifelse(is.na(lag(count)),NA,pmin(count,lag(count), na.rm=TRUE))) %>%
  mutate(notAB=ifelse(is.na(lag(count)),NA,
                      ifelse(count>lag(count),count-lag(count),0))) %>%
  mutate(AnotB=ifelse(is.na(lag(count)),NA,
                      ifelse(lag(count)>count,lag(count)-count,0))) %>%
  mutate(notAnotB=ifelse(is.na(lag(count)),NA,
                         ifelse(lag(count)==0 & count==0,1,0)))
  
alignment <- long_present %>% 
  mutate_at(vars(AB, notAB, AnotB, notAnotB),  
            function(x) as.numeric(x > 0)) %>% 
  gather(measure, value, AB, notAB, AnotB, notAnotB) %>% 
  group_by(session, person, measure) %>% 
  summarise(n = sum(value, na.rm = T)) %>%  
  spread(measure, n) %>% 
  mutate(A = (AB + AnotB) / (AB + AnotB + notAB + notAnotB), 
         B = (AB + notAB) / (AB + AnotB + notAB + notAnotB), 
         PAB= AB/(AB + notAB), 
         PAnotB = AnotB/(AnotB + notAnotB), 
         alignment = PAB - (PAB*A + PAnotB*(1-A))) %>% 
  select(person, session, alignment) %>% 
  filter(!is.na(alignment))

ggplot(data=alignment, aes(x=as.factor(session), y=alignment, group=person)) +
  geom_line(stat = 'identity', aes(color=person)) +theme_bw() +
  xlab('Session') +
  ylab('Linguistic Alignment')
   
# relabel probability things (flip AB)
# run the whole thing fully 
# push via git lfs
# basic alignment plot
# look out for stan stuff

# Now want to:
# 1. flatten all successive utterances from the same person
# 2. Count the number of appearances of each target word in each utterances
# 3. At a lag of one (use the lag function), check if the previous speaker's utterance contained each word
# 4. aggregate!
```
