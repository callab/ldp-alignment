---
title: "Linguistic Alignment in LDP"
author: "Jo Denby, Ashley Leung, and Dan Yurovksy"
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: no
  html_document:
    code_folding: show
    number_sections: no
    theme: lumen
    toc: no
    toc_float: no
---

```{r setup, include = FALSE}
library(knitr)
library(data.table)
library(tidyverse)
library(readr)
library(stringr)
library(DT)
library(tidytext)
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = TRUE, tidy = FALSE)
```


Read in LDP data. Don't display in public repo
```{r read_ldp}
LDP_DIR <- "~/Documents/LDP/ldp.db"
#LDP_DIR <- "~/ldp/data/ldp.db"

# Read in LDP data
ldp <- src_sqlite(LDP_DIR)

# Get all participants
subjs <- tbl(ldp, "subjects") %>%
  collect()

# Get visit data
visits <- tbl(ldp, "visits") %>%
  collect() %>%
  select(subject, session, date, child_age, child_age_years, child_age_months,
         income, mother_education, father_education)
```

Read in LIWC Categories
```{r read_liwc}
liwc <- read_tsv("word_lists/liwc2007_converted.tsv", col_names = FALSE) %>%
  rename(word = X1, category = X2)
```

Set up utterances for computing alignment
```{r setup_alignment}
# What do we do when both participants talk on the same line
utterances <- tbl(ldp, "utterances") %>%
  select(subject, session, line, p_chat, c_chat) %>%
  filter(p_chat != "" | c_chat != "") %>%
  collect() %>%
  mutate(order = 1:n())
  
#use rleid from data table to do squashing by run
split_utts <- utterances %>%
  gather(person, chat, c(p_chat, c_chat)) %>%
  mutate(person = if_else(person == "p_chat", "Parent", "Child")) %>%
  filter(chat != "") %>%
  arrange(order) %>%
  mutate(run = rleid(subject, session, person)) %>%
  group_by(run, subject, session, person) %>%
  summarise(chat = paste0(chat, collapse = " ")) 

#splits up run utterances into individual token words
tokens <- split_utts %>%
  unnest_tokens(word, chat) %>%
  arrange(run, subject, session) %>%
  group_by(subject, session)

# #WRONG for regular expressions
# # use grep
# present <- map(1:3, function(row) {
#   data_frame(present = tokens$word == as.character(liwc[row, "word"]))
# }) %>%
#   bind_cols() 

# ^ asserts the beginning of pattern, $ asserts end; works with regular expressions as well
# ~ 15min
present <- map(1:10, function(row) {
  data_frame(present = grepl(paste0("^",liwc[row,"word"],'$'), tokens$word,ignore.case = TRUE))}) %>%
  bind_cols()
names(present) <- pull(liwc,word)[1:10]

tokens_slice <- tokens[1:1000,]

# est time ~107 mins
long_present <- present %>%
    slice(1:1000) %>%
  bind_cols(tokens_slice) %>%
  gather(liwc_word, count, names(present)) %>%
  left_join(liwc, by = c("liwc_word" = "word")) %>%
  group_by(subject, session, person, category,run) %>%
  summarise(count = sum(count)) %>%
  arrange(run) %>%
  group_by(subject,session, category) %>%
  mutate(BA=ifelse(is.na(lag(count)),NA,pmin(count,lag(count), na.rm=TRUE))) %>%
  mutate(notBA=ifelse(is.na(lag(count)),NA,
                      ifelse(count>lag(count),count-lag(count),0))) %>%
  mutate(BnotA=ifelse(is.na(lag(count)),NA,
                      ifelse(lag(count)>count,lag(count)-count,0))) %>%
  mutate(notBnotA=ifelse(is.na(lag(count)),NA,
                         ifelse(lag(count)==0 & count==0,1,0)))


# # of base instances
# # of times a category hit follows another

# Now want to:
# 1. flatten all successive utterances from the same person
# 2. Count the number of appearances of each target word in each utterances
# 3. At a lag of one (use the lag function), check if the previous speaker's utterance contained each word
# 4. aggregate!
```
