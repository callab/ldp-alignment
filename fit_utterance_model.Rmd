---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(stringr)
library(rstan)
library(bayesplot)
library(tidyboot)
library(shinystan)
library(lme4)

library(feather)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```


```{r}
LDP_DIR <- "~/Documents/LDP/ldp.db"

LDP_DIR <- "~/ldp/data/ldp.db"

# Read in LDP data
ldp <- src_sqlite(LDP_DIR)

# utterances <- tbl(ldp, "utterances") %>%
#   select(subject, session, line, p_chat, c_chat) %>%
#   filter(p_chat != "" | c_chat != "") %>%
#   collect() %>%
#   mutate(order = 1:n()) 

############## UTTERANCES DF ###################
utterances <- read_feather("utterances.feather")
################################################

```

```{r}

# mean and sd length per session per session per subject
all_utt_lengths <- utterances %>%
  left_join(parentIDs) %>%
  filter(!is.na(p_chat)) %>%
  select(session,p_chat,subject_numeral) %>%
  mutate(length = str_count(p_chat, " ")) %>%
  filter(length > 0) %>%
  group_by(session, subject_numeral) %>%
  summarise(length = mean(length)) %>%
  summarise(sem = sd(length)/sqrt(n()-1),
            mean = mean(length))

#individual slopes for parents' mean utterance length
indiv_slopes <- utterances %>%
  left_join(parentIDs) %>%
  filter(!is.na(p_chat)) %>%
  select(session,p_chat,subject_numeral) %>%
  mutate(length = str_count(p_chat, " ")) %>%
  filter(length > 0) %>%
  group_by(subject_numeral, session) %>%
  summarise(length = mean(length, na.rm = T)) %>%
  filter(!is.na(length)) %>%
  group_by(subject_numeral) %>%
  spread(session, length) %>%
  mutate(slope = `12` - `1`) %>%
  filter(!is.na(slope))
  
cor.test(indiv_slopes$slope, indiv_slopes$`1`)

# mean utterance length for parents across development
ggplot(all_utt_lengths, aes(x = session, y =mean)) + 
  geom_pointrange(aes(ymin = mean - sem, ymax = mean +sem)) + 
  geom_line() + 
  theme(legend.position = "none")


###### DESCRIPTIVE STATISTICS ABOUT EMPIRICAL DATA #########
#number of sessions for each subject
n_session <- one_file_utts %>%
   group_by(subject_numeral, session) %>%
   distinct(mean_length) %>%
   group_by(subject_numeral) %>%
   summarise(n = n()) %>%
   filter(n >= 5)

#number of subjects for each session
n_subject <- one_file_utts %>%
   group_by(subject_numeral, session) %>%
   distinct(mean_length) %>%
   group_by(session) %>%
   summarise(n = n()) %>%
   filter(n >= 5)  

#empirical overdispersion and mean length per subject per session
over_disps <- one_file_utts %>%
   group_by(subject_numeral, session) %>%
   summarise(var_length = var(length),
            mean_length = mean(length)) %>% 
   mutate(overdisp = var_length - mean_length) %>%
   filter(subject_numeral %in% n_session$subject_numeral) %>%
   gather(measure, value, mean_length, overdisp)

#overdispersion for each session averaged across subjects
indiv_over_disps <- over_disps %>%
  filter(measure == "overdisp") %>%
  group_by(session, subject_numeral) %>%
  summarise(overdisp = mean(value)) %>%
  summarise(overdisp = mean(overdisp))
 
#regression model for overdispersion
over_disp_lm <- lmer(value ~ session + (1|subject_numeral), data = filter(over_disps, measure == "overdisp"))

############################################################


# pdf("over_disps.pdf", width = 10, height = 10)
# over_disps %>%
#   ggplot(aes(x = session, y = value, color = measure,
#              group = interaction(as.factor(subject_numeral), measure))) +
#   facet_wrap(~ as.factor(subject_numeral)) +
#   geom_line() +
#   geom_smooth(method = "lm", color = "black", size = .1) + 
#   theme(legend.position = "none")
# dev.off()

  # one_file_utts %>%
  # group_by(session, subject_numeral) %>%
  # distinct(mean_length) %>%
  # filter(subject_numeral %in% n_session$subject_numeral) %>%
  # ggplot(aes(x = session, y = mean_length, color = as.factor(subject_numeral),
  #            group = as.factor(subject_numeral))) +
  # facet_wrap(~ as.factor(subject_numeral)) +
  # geom_line() +
  #   geom_smooth(method = "lm", color = "black") +
  # theme(legend.position = "none")


## FILTERING AND PREPARING FOR STAN MODEL ##
one_file_utts <- utterances %>%
  filter(!is.na(p_chat)) %>%
  select(session,p_chat,subject) %>%
  mutate(length = str_count(p_chat, " ")) %>%
  filter(length > 0)

# enumerate parent IDs after filtering so indices match up in stan code
parentIDs <-  cbind(unique(one_file_utts$subject), seq(1,length(unique(one_file_utts$subject)))) %>%
  as.data.frame()
colnames(parentIDs) <- c('subject', 'subject_numeral')

one_file_utts <- one_file_utts %>%
  left_join(parentIDs) %>%
  mutate(mean_length = mean(length)) %>%
  mutate(session = session - 6) %>%
  group_by(subject_numeral, session) %>%
  #filter(subject_numeral %in% c(1,2)) %>%
  #filter(session %in% c(-1,0,1)) %>%
  ungroup() 


############################################

plot(one_file_utts[subject_numeral==1]$mean_length)
# distribution of utterance lengths

######## STAN DATA ########################
utt_parent <- one_file_utts$subject_numeral
numParents <- length(unique(utt_parent))
numLengths <- length(one_file_utts$length)
utt_length <- one_file_utts$length
utt_session <- one_file_utts$session
parent_indices <- match(unique(utt_parent), utt_parent)
###########################################

################ RUN MODEL ################
fit <- stan(file = "utt_length.stan", chains = 1)
###########################################


launch_shinystan(fit)

posterior <- as.matrix(fit)

##### DO THIS TO READ IN THE FIT AND EXAMINE USING SHINYSTAN #####
# for all parents in session 5
fit_1 <- readRDS("fit_1.rds")

launch_shinystan(fit_1)

# for all parents in all sessions
fit_big <- readRDS("fit_big.rds")

launch_shinystan(fit_big)

#for all parents - estimate linear model
fit_really_big <- readRDS("fit_really_big.rds")

launch_shinystan(fit_really_big)
##################################################################

over_ps <- extract(fit, "alpha_mean_p")$alpha_mean_p %>%
  as_data_frame() %>%
  bind_cols(lp = extract(fit, "lp__")$lp__)
  


## Get model summary stats ##
fit_summary <- summary(fit)

fit_sum_df <- as.data.frame(fit_summary$summary) %>%
  tibble::rownames_to_column(var = 'par')

parent_over_slopes <- fit_sum_df%>%
  filter(grepl("^alpha_over_p",fit_sum_df$par)) %>%
  select(par, mean) %>%
  mutate(subject_numeral= c(1))

parent_mean_slopes <- fit_sum_df %>%
  filter(grepl("^alpha_mean_p",fit_sum_df$par)) %>%
  select(par, mean) 
  
parent_mean_intercepts <- fit_sum_df %>%
  filter(grepl("^mu_p",fit_sum_df$par)) %>%
  select(par, mean) %>%
  mutate(subject_numeral = c(1))

parent_overdisp_intercepts <- fit_sum_df %>%
  filter(grepl("^over_p", fit_sum_df$par)) %>%
  select(par, mean) %>%
  mutate(subject_numeral = c(1))

  #mutate(subject_numeral= unique(over_disps$subject_numeral))

#empirical overdispersion and mean length per subject per session
over_disps <- one_file_utts %>%
   group_by(subject_numeral, session) %>%
   summarise(var_length = var(length),
            mean_length = mean(length)) %>% 
   mutate(overdisp = ((mean_length)^2)/(var_length - mean_length)) %>%
   #filter(subject_numeral %in% utt_parent) %>%
   gather(measure, value, mean_length, overdisp)

#empirical means for parents across sessions of interest
empirical_means <- over_disps %>%
  group_by(measure, subject_numeral) %>%
  #filter(session %in% c(5,6,7)) %>%
  summarise(value = mean(value, na.rm = T)) %>%
  mutate(type = "empirical")

parent_stats <- left_join(parent_overdisp_intercepts, parent_mean_intercepts, 'subject_numeral') %>%
  rename(mean_length = mean.y, 
         overdisp = mean.x) %>%
  select(subject_numeral, mean_length, overdisp) %>%
  gather(measure, value, mean_length, overdisp) %>%
  mutate(type = "model") %>%
  bind_rows(empirical_means) %>%
  spread(type, value)
  

ggplot(parent_stats, aes(x = model, y = empirical)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~ measure, scales = "free")
#######################

# plot_title <- ggtitle("Posterior distributions",
#                       "with medians and 80% intervals")
# mcmc_areas(posterior, 
#            pars = c("mu_mean", "mu_over", "mu_p[1]","mu_p[2]","mu_p[3]", "mu_p[4]", "over_p[1]",
#                     "over_p[2]", "over_p[3]", 'over_p[4]'), 
#            prob = 0.8) + plot_title


posterior2 <- extract(fit_big, inc_warmup = TRUE, permuted = FALSE)


color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("betapop", "alphapop", "betapar[1]", "alphapar[1]", "lp__"),
                n_warmup = 300,
                facet_args = list(nrow = 2, labeller = label_parsed))
p + facet_text(size = 15)



color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("mu_mean", "sigma_mean",
                                      "mu_sd", "sigma_sd"),
                n_warmup = 300,
                facet_args = list(nrow = 2, labeller = label_parsed))
p + facet_text(size = 15)


hypothetical <- rexp(10000, .25)


data_frame(x = seq(.1, 2.1, .1), 
           p =  sapply(x, function(x) {sum(dexp(lengths, x,  log = TRUE))}))


hist(hypothetical)

color_scheme_set("red")
ppc_dens_overlay(y = fit$length, 
                 yrep = posterior_predict(fit, draws = 50))

vals <- extract(fit, pars = c("alphapar[3]", "alphapar[1]","alphapar[2]", "alphapar[4]", "betapar[1]","betapar[2]","betapar[3]", "betapar[4]")) %>%
  bind_rows() %>%
  summarise_all(mean) %>%
  gather(parameter, value) %>%
  separate(parameter, into = c("parameter", "person", sep = "[")) %>%
  select(-`[`) %>%
  spread(parameter, value) %>%
  mutate(mean = alphapar/betapar,
         var = (alphapar/(betapar^2))*(betapar+1),
         phi = (mean^2)/(var-mean))
```

