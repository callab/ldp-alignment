---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(stringr)
library(rstan)
library(bayesplot)
library(tidyboot)
library(shinystan)

library(feather)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```


```{r}
LDP_DIR <- "~/Documents/LDP/ldp.db"

LDP_DIR <- "~/ldp/data/ldp.db"

# Read in LDP data
ldp <- src_sqlite(LDP_DIR)

# utterances <- tbl(ldp, "utterances") %>%
#   select(subject, session, line, p_chat, c_chat) %>%
#   filter(p_chat != "" | c_chat != "") %>%
#   collect() %>%
#   mutate(order = 1:n()) 

############## UTTERANCES DF ###################
utterances <- read_feather("utterances.feather")
################################################

```

```{r}

all_utt_lengths <- utterances %>%
  left_join(parentIDs) %>%
  filter(!is.na(p_chat)) %>%
  select(session,p_chat,subject_numeral) %>%
  mutate(length = str_count(p_chat, " ")) %>%
  filter(length > 0) %>%
  group_by(session, subject_numeral) %>%
  summarise(length = mean(length)) %>%
  summarise(sem = sd(length)/sqrt(n()-1),
            mean = mean(length))
# mean and sd length per session per session per subject

indiv_slopes <- utterances %>%
  left_join(parentIDs) %>%
  filter(!is.na(p_chat)) %>%
  select(session,p_chat,subject_numeral) %>%
  mutate(length = str_count(p_chat, " ")) %>%
  filter(length > 0) %>%
  group_by(subject_numeral, session) %>%
  summarise(length = mean(length, na.rm = T)) %>%
  filter(!is.na(length)) %>%
  group_by(subject_numeral) %>%
  spread(session, length) %>%
  mutate(slope = `12` - `1`) %>%
  filter(!is.na(slope))
  
cor.test(indiv_slopes$slope, indiv_slopes$`1`)


ggplot(all_utt_lengths, aes(x = session, y =mean)) + 
  geom_pointrange(aes(ymin = mean - sem, ymax = mean +sem)) + 
  geom_line() + 
  theme(legend.position = "none")

# mean utterance length for parents across development




  


# n_session <- one_file_utts %>%
#   group_by(subject_numeral, session) %>%
#   distinct(mean_length) %>%
#   group_by(subject_numeral) %>%
#   summarise(n = n()) %>%
#   filter(n >= 5)


# over_disps <- one_file_utts %>%
#   group_by(subject_numeral, session) %>%
#   summarise(var_length = var(length),
#            mean_length = mean(length)) %>%
#   mutate(overdisp = var_length - mean_length)  %>%
#   filter(subject_numeral %in% n_session$subject_numeral) %>%
#   gather(measure, value, mean_length, overdisp)

# pdf("over_disps.pdf", width = 10, height = 10)
# over_disps %>%
#   ggplot(aes(x = session, y = value, color = measure,
#              group = interaction(as.factor(subject_numeral), measure))) +
#   facet_wrap(~ as.factor(subject_numeral)) +
#   geom_line() +
#   geom_smooth(method = "lm", color = "black", size = .1) + 
#   theme(legend.position = "none")
# dev.off()

  # one_file_utts %>%
  # group_by(session, subject_numeral) %>%
  # distinct(mean_length) %>%
  # filter(subject_numeral %in% n_session$subject_numeral) %>%
  # ggplot(aes(x = session, y = mean_length, color = as.factor(subject_numeral),
  #            group = as.factor(subject_numeral))) +
  # facet_wrap(~ as.factor(subject_numeral)) +
  # geom_line() +
  #   geom_smooth(method = "lm", color = "black") +
  # theme(legend.position = "none")


## FILTERING AND PREPARING FOR STAN MODEL ##
one_file_utts <- utterances %>%
  filter(!is.na(p_chat)) %>%
  select(session,p_chat,subject) %>%
  mutate(length = str_count(p_chat, " ")) %>%
  filter(length > 0)

# enumerate parent IDs after filtering so indices match up in stan code
parentIDs <-  cbind(unique(one_file_utts$subject), seq(1,length(unique(one_file_utts$subject)))) %>%
  as.data.frame()
colnames(parentIDs) <- c('subject', 'subject_numeral')

one_file_utts <- one_file_utts %>%
  left_join(parentIDs) %>%
  mutate(mean_length = mean(length)) %>%
  mutate(session = session - 6) %>%
  group_by(subject_numeral, session)

############################################

plot(one_file_utts[subject_numeral==1]$mean_length)
# distribution of utterance lengths

######## STAN DATA ########################
utt_parent <- one_file_utts$subject_numeral
numParents <- length(unique(utt_parent))
numLengths <- length(one_file_utts$length)
utt_length <- one_file_utts$length
utt_session <- one_file_utts$session
###########################################

## RUN MODEL ##
fit <- stan(file = "utt_length.stan", chains = 1)
###############

launch_shinystan(fit)

posterior <- as.matrix(fit)

##### DO THIS TO READ IN THE FIT AND EXAMINE USING SHINYSTAN #####
# for all parents in session 5
fit_1 <- readRDS("fit_1.rds")

launch_shinystan(fit_1)

# for all parents in all sessions
fit_big <- readRDS("fit_big.rds")

launch_shinystan(fit_big)

#for all parents - estimate linear model
fit_really_big <- readRDS("fit_really_big.rds")

launch_shinystan(fit_really_big)
##################################################################


# plot_title <- ggtitle("Posterior distributions",
#                       "with medians and 80% intervals")
# mcmc_areas(posterior, 
#            pars = c("mu_mean", "mu_over", "mu_p[1]","mu_p[2]","mu_p[3]", "mu_p[4]", "over_p[1]",
#                     "over_p[2]", "over_p[3]", 'over_p[4]'), 
#            prob = 0.8) + plot_title


posterior2 <- extract(fit_big, inc_warmup = TRUE, permuted = FALSE)


color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("betapop", "alphapop", "betapar[1]", "alphapar[1]", "lp__"),
                n_warmup = 300,
                facet_args = list(nrow = 2, labeller = label_parsed))
p + facet_text(size = 15)



color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("mu_mean", "sigma_mean",
                                      "mu_sd", "sigma_sd"),
                n_warmup = 300,
                facet_args = list(nrow = 2, labeller = label_parsed))
p + facet_text(size = 15)


hypothetical <- rexp(10000, .25)


data_frame(x = seq(.1, 2.1, .1), 
           p =  sapply(x, function(x) {sum(dexp(lengths, x,  log = TRUE))}))


hist(hypothetical)

color_scheme_set("red")
ppc_dens_overlay(y = fit$length, 
                 yrep = posterior_predict(fit, draws = 50))

vals <- extract(fit, pars = c("alphapar[3]", "alphapar[1]","alphapar[2]", "alphapar[4]", "betapar[1]","betapar[2]","betapar[3]", "betapar[4]")) %>%
  bind_rows() %>%
  summarise_all(mean) %>%
  gather(parameter, value) %>%
  separate(parameter, into = c("parameter", "person", sep = "[")) %>%
  select(-`[`) %>%
  spread(parameter, value) %>%
  mutate(mean = alphapar/betapar,
         var = (alphapar/(betapar^2))*(betapar+1),
         phi = (mean^2)/(var-mean))
```

